This project builds a facial emotion recognition model, and integrates it to a web application that provide music therapy service.
The application uses machine learning and the PERN stack (PostgreSQL, Express, React and Node.js) to identify user's emotional states from their facial expressions, recommend and generate music playlist that aligns with their current emotion.
The aim is to make music therapy more widely available and a useful tool for assistance outside of traditional settings. 
The machine learning model was trained with FER2013 and CK+ dataset to ensure it has the capability of emotion detection.
Then it is enhanced with transfer learning technique to ensure broad demographic applicability and accuracy in emotion detection.
The application's capability to correctly identify emotions and provide music recommendation was confirmed by the initial testing.
It also brought attention to difficulties with expanding the application to support more different demographic users and integrating other music services.
Future improvements will concentrate on expanding the scope of service integrations, and enhancing scalability and integrating user feedback to improve the recommendation algorithms.
This report outlines the project's scope, from conception to testing and assessment, and discusses the potential future developments that could improve the application's contribution on mental health support. 
This project serves as an example of how technology can link in mental healthcare by providing a scalable, personalized solution that meet individual emotional requirements.