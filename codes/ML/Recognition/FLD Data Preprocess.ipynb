{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 02:47:26.911884: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-28 02:47:26.911944: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-28 02:47:26.912596: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-28 02:47:26.916665: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from pymongo import MongoClient\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "K.clear_session()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing into MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"admin\"\n",
    "password = \"g41WZadbgsPmC37B\"\n",
    "uri = f\"mongodb+srv://{user}:{password}@rpm.spzvwtw.mongodb.net\"\n",
    "db_name = 'RecognitivePretrainedModels'\n",
    "collection = '300W'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Connected!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    connection = MongoClient(uri)\n",
    "    db = connection[db_name]\n",
    "    data = db[collection]\n",
    "    print(\"Database Connected!\")\n",
    "except:\n",
    "    print(\"Connection failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_image(image_path):\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        image_data = image_file.read()\n",
    "        serialized_image = pickle.dumps(image_data)\n",
    "    return serialized_image\n",
    "\n",
    "def parse_pts(filename):\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "        start = lines.index(\"{\\n\") + 1  # Find the start of the landmarks\n",
    "        landmarks = [list(map(float, lines[i].strip().split())) for i in range(start, start + 68)]\n",
    "        return landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_insert_data(directory, collection):\n",
    "    for image_path in glob.glob(os.path.join(directory, \"*.png\")):\n",
    "        pts_path = image_path.replace(\".png\", \".pts\")\n",
    "\n",
    "        if os.path.exists(pts_path):\n",
    "            try:\n",
    "                serialized_image = serialize_image(image_path)\n",
    "                landmarks = parse_pts(pts_path)\n",
    "\n",
    "                document = {\n",
    "                    \"image\": serialized_image,\n",
    "                    \"landmarks\": landmarks\n",
    "                }\n",
    "\n",
    "                collection.insert_one(document)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {image_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, size=(224, 224)):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale\n",
    "    image = cv2.resize(image, size)  # Resize image\n",
    "    image = image / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pts(filename):\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "        start = lines.index(\"{\\n\") + 1  # Find the start of the landmarks\n",
    "        end = lines.index(\"}\\n\", start)  # Find the end of the landmarks\n",
    "        landmarks = [list(map(float, line.strip().split())) for line in lines[start:end]]\n",
    "        return np.array(landmarks).reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_landmarks(landmarks, image_size=(224, 224)):\n",
    "    normalized_landmarks = []\n",
    "    for landmark in landmarks:\n",
    "        landmark = landmark.reshape(-1, 2)\n",
    "        # Normalize each point individually\n",
    "        norm_landmark = [[x / image_size[0], y / image_size[1]] for x, y in landmark]\n",
    "        normalized_landmarks.append(norm_landmark)\n",
    "    return np.array(normalized_landmarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(directory):\n",
    "    images = []\n",
    "    landmarks = []\n",
    "\n",
    "    for image_path in glob.glob(os.path.join(directory, \"*.png\")):\n",
    "        image = load_image(image_path)\n",
    "        pts_path = image_path.replace(\".png\", \".pts\")\n",
    "        landmark = parse_pts(pts_path)\n",
    "        landmark = normalize_landmarks(landmark)\n",
    "\n",
    "        images.append(image)\n",
    "        landmarks.append(landmark)\n",
    "\n",
    "    return np.array(images), np.array(landmarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, landmark_annotations = preprocess_data(\"/home/zachcyn/dev/fyp/codes/Dataset/300W\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(images, landmarks, test_size=0.2):\n",
    "    return train_test_split(images, landmarks, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = split_data(images, landmark_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 02:50:31.911622: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:50:31.928876: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:50:31.928948: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:50:31.930855: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:50:31.930900: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:50:31.930926: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:50:32.112381: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:50:32.112440: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:50:32.112448: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-12-28 02:50:32.112462: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-12-28 02:50:32.112611: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:50:32.112633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3600 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-12-28 02:50:32.114936: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 192675840 exceeds 10% of free system memory.\n",
      "2023-12-28 02:50:32.300492: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 192675840 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"landmark_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 1)]     0         \n",
      "                                                                 \n",
      " normalization (Normalizati  (None, 224, 224, 1)       3         \n",
      " on)                                                             \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      320       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 222, 222, 32)      128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 111, 111, 32)      0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 109, 109, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 107, 107, 64)      36928     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 107, 107, 64)      256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 53, 53, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 51, 51, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 51, 51, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 49, 49, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 49, 49, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 24, 24, 64)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 22, 22, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 22, 22, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 20, 20, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 20, 20, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 10, 10, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 8, 8, 256)         1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 16384)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              16778240  \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 1024)              4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 136)               139400    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17571147 (67.03 MB)\n",
      "Trainable params: 17567496 (67.01 MB)\n",
      "Non-trainable params: 3651 (14.27 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization, Input, Normalization\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def build_landmark_model(input_shape, output_size):\n",
    "    # Preprocessing layers\n",
    "    preprocess = Normalization()\n",
    "\n",
    "    # Convolutional layers\n",
    "    conv_1 = Conv2D(filters=32, kernel_size=(3, 3), activation='relu')\n",
    "    conv_2 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')\n",
    "    conv_3 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')\n",
    "    conv_4 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')\n",
    "    conv_5 = Conv2D(filters=64, kernel_size=(3, 3), activation='relu')\n",
    "    conv_6 = Conv2D(filters=128, kernel_size=(3, 3), activation='relu')\n",
    "    conv_7 = Conv2D(filters=128, kernel_size=(3, 3), activation='relu')\n",
    "    conv_8 = Conv2D(filters=256, kernel_size=(3, 3), activation='relu')\n",
    "\n",
    "    # Pooling layers\n",
    "    pool_1 = MaxPooling2D(pool_size=(2, 2))\n",
    "    pool_2 = MaxPooling2D(pool_size=(2, 2))\n",
    "    pool_3 = MaxPooling2D(pool_size=(2, 2))\n",
    "    pool_4 = MaxPooling2D(pool_size=(2, 2))\n",
    "\n",
    "    # Batch normalization layers\n",
    "    bn_1 = BatchNormalization()\n",
    "    bn_2 = BatchNormalization()\n",
    "    bn_3 = BatchNormalization()\n",
    "    bn_4 = BatchNormalization()\n",
    "    bn_5 = BatchNormalization()\n",
    "    bn_6 = BatchNormalization()\n",
    "    bn_7 = BatchNormalization()\n",
    "    bn_8 = BatchNormalization()\n",
    "    bn_9 = BatchNormalization()\n",
    "\n",
    "    # Dense layers\n",
    "    dense_1 = Dense(units=1024, activation='relu')\n",
    "    dense_2 = Dense(units=output_size)\n",
    "\n",
    "    # Flatten layer\n",
    "    flatten = Flatten()\n",
    "\n",
    "    # Define the model\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = preprocess(inputs)\n",
    "    x = conv_1(x)\n",
    "    x = bn_1(x)\n",
    "    x = pool_1(x)\n",
    "    x = conv_2(x)\n",
    "    x = bn_2(x)\n",
    "    x = conv_3(x)\n",
    "    x = bn_3(x)\n",
    "    x = pool_2(x)\n",
    "    x = conv_4(x)\n",
    "    x = bn_4(x)\n",
    "    x = conv_5(x)\n",
    "    x = bn_5(x)\n",
    "    x = pool_3(x)\n",
    "    x = conv_6(x)\n",
    "    x = bn_6(x)\n",
    "    x = conv_7(x)\n",
    "    x = bn_7(x)\n",
    "    x = pool_4(x)\n",
    "    x = conv_8(x)\n",
    "    x = bn_8(x)\n",
    "    x = flatten(x)\n",
    "    x = dense_1(x)\n",
    "    x = bn_9(x)\n",
    "    outputs = dense_2(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs, name=\"landmark_model\")\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "input_shape = (224, 224, 1)  # Example input shape, adjust as needed\n",
    "output_size = 136  # Example output size, adjust as needed\n",
    "model = build_landmark_model(input_shape, output_size)\n",
    "model.summary()  # Display the model architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 02:50:47.520963: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 192675840 exceeds 10% of free system memory.\n",
      "2023-12-28 02:50:47.761511: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 192675840 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 02:50:48.355542: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 192675840 exceeds 10% of free system memory.\n",
      "2023-12-28 02:50:50.302624: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8907\n",
      "2023-12-28 02:50:51.809173: I external/local_xla/xla/service/service.cc:168] XLA service 0x7f2194fcf1b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-28 02:50:51.809207: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 3060 Laptop GPU, Compute Capability 8.6\n",
      "2023-12-28 02:50:51.814001: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1703703051.896732   27868 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 11s 138ms/step - loss: 17.8208 - accuracy: 0.0083 - val_loss: 18.8746 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 1s 76ms/step - loss: 15.6535 - accuracy: 0.0021 - val_loss: 23.1214 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 13.9926 - accuracy: 0.0146 - val_loss: 24.2152 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 12.0621 - accuracy: 0.0417 - val_loss: 19.4122 - val_accuracy: 0.0000e+00\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 10.0917 - accuracy: 0.0125 - val_loss: 9.1715 - val_accuracy: 0.0167\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 7.7338 - accuracy: 0.0229 - val_loss: 9.1375 - val_accuracy: 0.0000e+00\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 6.1604 - accuracy: 0.0458 - val_loss: 8.0536 - val_accuracy: 0.0000e+00\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 4.6577 - accuracy: 0.0542 - val_loss: 31.9045 - val_accuracy: 0.0000e+00\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 1s 72ms/step - loss: 3.8178 - accuracy: 0.0667 - val_loss: 15.7291 - val_accuracy: 0.0000e+00\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 3.7317 - accuracy: 0.0896 - val_loss: 29.1136 - val_accuracy: 0.0000e+00\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 3.5021 - accuracy: 0.1146 - val_loss: 17.0980 - val_accuracy: 0.0000e+00\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 3.4807 - accuracy: 0.1292 - val_loss: 9.7536 - val_accuracy: 0.1833\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 3.2311 - accuracy: 0.1208 - val_loss: 18.7346 - val_accuracy: 0.0000e+00\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 3.1105 - accuracy: 0.1479 - val_loss: 13.0520 - val_accuracy: 0.0000e+00\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.8407 - accuracy: 0.1292 - val_loss: 9.8097 - val_accuracy: 0.0750\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.7773 - accuracy: 0.1229 - val_loss: 15.5115 - val_accuracy: 0.0500\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.7342 - accuracy: 0.1667 - val_loss: 12.9648 - val_accuracy: 0.0750\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.5949 - accuracy: 0.1875 - val_loss: 8.9054 - val_accuracy: 0.0583\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.7703 - accuracy: 0.1521 - val_loss: 13.2757 - val_accuracy: 0.0000e+00\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 2.8036 - accuracy: 0.1479 - val_loss: 15.6524 - val_accuracy: 0.2417\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.8255 - accuracy: 0.1583 - val_loss: 10.9606 - val_accuracy: 0.2333\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.7014 - accuracy: 0.1250 - val_loss: 9.3520 - val_accuracy: 0.1083\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.8357 - accuracy: 0.1750 - val_loss: 13.5203 - val_accuracy: 0.1333\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.8383 - accuracy: 0.1896 - val_loss: 14.6514 - val_accuracy: 0.1333\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.7361 - accuracy: 0.1562 - val_loss: 7.7436 - val_accuracy: 0.0583\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.5245 - accuracy: 0.1708 - val_loss: 8.2561 - val_accuracy: 0.1667\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 3.2005 - accuracy: 0.1500 - val_loss: 7.7533 - val_accuracy: 0.0250\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.6924 - accuracy: 0.1646 - val_loss: 9.8938 - val_accuracy: 0.1333\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.7121 - accuracy: 0.1854 - val_loss: 10.1340 - val_accuracy: 0.0167\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.4108 - accuracy: 0.1771 - val_loss: 8.8388 - val_accuracy: 0.2500\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.6672 - accuracy: 0.2021 - val_loss: 8.6711 - val_accuracy: 0.1167\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.5248 - accuracy: 0.1771 - val_loss: 7.3523 - val_accuracy: 0.2083\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.7164 - accuracy: 0.1688 - val_loss: 7.0826 - val_accuracy: 0.2500\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 1s 73ms/step - loss: 2.5142 - accuracy: 0.1854 - val_loss: 7.5965 - val_accuracy: 0.1667\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.2906 - accuracy: 0.1958 - val_loss: 6.7191 - val_accuracy: 0.2333\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.4215 - accuracy: 0.1771 - val_loss: 5.9285 - val_accuracy: 0.2417\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.4248 - accuracy: 0.1833 - val_loss: 6.0472 - val_accuracy: 0.1917\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.5038 - accuracy: 0.1958 - val_loss: 5.9483 - val_accuracy: 0.1750\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.4893 - accuracy: 0.2083 - val_loss: 6.3817 - val_accuracy: 0.1250\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.3522 - accuracy: 0.1708 - val_loss: 5.9746 - val_accuracy: 0.1250\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.5167 - accuracy: 0.1625 - val_loss: 5.7416 - val_accuracy: 0.1417\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.4705 - accuracy: 0.2042 - val_loss: 6.8649 - val_accuracy: 0.1167\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 1s 77ms/step - loss: 2.4275 - accuracy: 0.1750 - val_loss: 6.2485 - val_accuracy: 0.1417\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 1s 80ms/step - loss: 2.5539 - accuracy: 0.1979 - val_loss: 6.1458 - val_accuracy: 0.1833\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 1s 74ms/step - loss: 2.5203 - accuracy: 0.1917 - val_loss: 6.6010 - val_accuracy: 0.1333\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 1s 81ms/step - loss: 2.6186 - accuracy: 0.1958 - val_loss: 7.4341 - val_accuracy: 0.1833\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 1s 75ms/step - loss: 2.7191 - accuracy: 0.2208 - val_loss: 7.5172 - val_accuracy: 0.1250\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 1s 83ms/step - loss: 2.2920 - accuracy: 0.2333 - val_loss: 6.7987 - val_accuracy: 0.1083\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 1s 82ms/step - loss: 2.5696 - accuracy: 0.1875 - val_loss: 6.3505 - val_accuracy: 0.1167\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 1s 84ms/step - loss: 2.3039 - accuracy: 0.1813 - val_loss: 7.8240 - val_accuracy: 0.1167\n"
     ]
    }
   ],
   "source": [
    "# Reshape y_train and y_test to have 136 values per sample\n",
    "y_train_flat = y_train.reshape(y_train.shape[0], -1)\n",
    "y_test_flat = y_test.reshape(y_test.shape[0], -1)\n",
    "\n",
    "# Now create the train_dataset with the reshaped y_train\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train_flat))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size=32)\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(train_dataset, epochs=50, validation_data=(X_test, y_test_flat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape: (120, 224, 224)\n",
      "y_test shape: (120, 68, 1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
