{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa4be944-e8f2-4788-bc8b-4d72da593d0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-29 02:46:04.007666: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-29 02:46:04.007717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-29 02:46:04.008573: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-29 02:46:04.017061: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools\n",
    "from pymongo import MongoClient\n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras_tuner import RandomSearch\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f89c6b3f-fb79-4b54-8c7a-3ced47c749d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"admin\"\n",
    "password = \"g41WZadbgsPmC37B\"\n",
    "uri = f\"mongodb+srv://{user}:{password}@rpm.spzvwtw.mongodb.net\"\n",
    "db_name = 'RecognitivePretrainedModels'\n",
    "train_collection = 'FER2013_TRAIN_MODIFIED'\n",
    "test_collection = 'FER2013_TEST_MODIFIED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c3dbd42-6a1c-4994-9453-273306d9491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Connected!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    connection = MongoClient(uri)\n",
    "    db = connection[db_name]\n",
    "    train_data = db[train_collection]\n",
    "    test_data = db[test_collection]\n",
    "    print(\"Database Connected!\")\n",
    "except:\n",
    "    print(\"Connection failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2572f3eb-3762-4893-b904-32405c72e825",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_data.find({})\n",
    "test = test_data.find({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969f9484-2c4e-4870-a6d9-d3d7b789c552",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20289258-e3de-46ef-ae28-20d305db4565",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(list(train))\n",
    "test_df = pd.DataFrame(list(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1cd9a5c-d28d-463c-b683-2e386baa6a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['_id'], axis=1, inplace=True)\n",
    "test_df.drop(['_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0486cd19-5405-4f20-a201-dfcac5135fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deserialize_image(img):\n",
    "  return pickle.loads(img)\n",
    "\n",
    "train_df[\"image\"] = train_df[\"image\"].apply(deserialize_image)\n",
    "test_df[\"image\"] = test_df[\"image\"].apply(deserialize_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bd286ac-920e-413e-90b8-65c43a4c0b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(img):\n",
    "    return img / 255.0\n",
    "\n",
    "train_df['image'] = train_df['image'].apply(normalize_image)\n",
    "test_df['image'] = test_df['image'].apply(normalize_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0575f79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(train_df['label'])  # Replace 'your_labels' with the original label names\n",
    "label_mapping = dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07d2688e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "int_encoded = label_encoder.fit_transform(train_df['label'])\n",
    "one_hot_encoded = to_categorical(int_encoded)\n",
    "train_df['encoded_label'] = list(one_hot_encoded)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "int_encoded = label_encoder.fit_transform(test_df['label'])\n",
    "one_hot_encoded = to_categorical(int_encoded)\n",
    "test_df['encoded_label'] = list(one_hot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6b505d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_df['image'].tolist())\n",
    "y = np.array(train_df['encoded_label'].tolist())\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c0653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(test_df['image'].tolist())\n",
    "y_test = np.array(test_df['encoded_label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f6b5009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 02:59:33.320472: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:59:33.325475: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:59:33.325519: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:59:33.327877: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:59:33.327922: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:59:33.327948: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:59:33.458443: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:59:33.458497: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:59:33.458504: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-12-28 02:59:33.458516: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:236] Using CUDA malloc Async allocator for GPU: 0\n",
      "2023-12-28 02:59:33.458634: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-12-28 02:59:33.458656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3600 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "2023-12-28 02:59:33.461058: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 398536704 exceeds 10% of free system memory.\n",
      "2023-12-28 02:59:33.791545: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 398536704 exceeds 10% of free system memory.\n",
      "2023-12-28 02:59:33.967758: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 170809344 exceeds 10% of free system memory.\n",
      "2023-12-28 02:59:34.062616: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 170809344 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size=32)\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = val_dataset.shuffle(buffer_size=1024).batch(batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef469ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = np.argmax(y_train, axis=1)\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(class_labels), y=class_labels)\n",
    "class_weight_dict = dict(enumerate(class_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65589d4",
   "metadata": {},
   "source": [
    "# Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124e81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(hp.Int('conv_1_filters', min_value=32, max_value=64, step=32), \n",
    "                     kernel_size=hp.Choice('conv_1_kernel', values=[3, 5]),\n",
    "                     activation='relu', input_shape=(48, 48, 1)))\n",
    "    model.add(Conv2D(hp.Int('conv_2_filters', min_value=64, max_value=128, step=32), \n",
    "                     kernel_size=hp.Choice('conv_2_kernel', values=[3, 5]), \n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(hp.Float('dropout_1', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Conv2D(hp.Int('conv_3_filters', min_value=128, max_value=256, step=32), \n",
    "                     kernel_size=hp.Choice('conv_3_kernel', values=[3, 5]), \n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(hp.Int('conv_4_filters', min_value=128, max_value=256, step=32), \n",
    "                     kernel_size=hp.Choice('conv_4_kernel', values=[3, 5]), \n",
    "                     activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(hp.Float('dropout_2', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(hp.Int('dense_units', min_value=512, max_value=1024, step=256), activation='relu'))\n",
    "    model.add(Dropout(hp.Float('dropout_3', min_value=0.0, max_value=0.5, step=0.1)))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'sgd']),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9213f-1bb2-40e7-9df5-754a5685c81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=30,\n",
    "    directory='new_output',\n",
    "    project_name='hparam_tuning'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948d7fa8-b885-48dc-ba71-85f2371e84a0",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e868b82-44f1-41e0-9551-91f57b4d6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1, l2\n",
    "\n",
    "def build_grid_model(conv_1_filters, conv_2_filters, conv_3_filters, conv_4_filters, \n",
    "                     conv_1_kernel, conv_2_kernel, conv_3_kernel, conv_4_kernel,\n",
    "                     dropout_1, dropout_2, dropout_3, dense_units, optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(conv_1_filters, kernel_size=conv_1_kernel, activation='relu', input_shape=(48,48,1)))\n",
    "    model.add(Conv2D(conv_2_filters, kernel_size=conv_2_kernel, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(dropout_1))\n",
    "\n",
    "    model.add(Conv2D(conv_3_filters, kernel_size=conv_3_kernel, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Conv2D(conv_4_filters, kernel_size=conv_4_kernel, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(dropout_2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_units, activation='relu', kernel_regularizer=l1(0.01)))\n",
    "    model.add(Dropout(dropout_3))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84446608-7ed2-4c9a-bdd0-de0ab187ee30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 23:22:26.546868: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_2/dropout_6/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 128, 128, 3, 3, 3, 3, 0.1, 0.0, 0.0, 512, 'adam', 32, 10) done\n",
      "Best Score: 0.5725693106651306\n",
      "Best Params: {'conv_1_filters': 32, 'conv_2_filters': 64, 'conv_3_filters': 128, 'conv_4_filters': 128, 'conv_1_kernel': 3, 'conv_2_kernel': 3, 'conv_3_kernel': 3, 'conv_4_kernel': 3, 'dropout_1': 0.1, 'dropout_2': 0.0, 'dropout_3': 0.0, 'dense_units': 512, 'optimizer': 'adam'}\n",
      "(32, 64, 128, 128, 3, 3, 3, 3, 0.1, 0.0, 0.0, 768, 'adam', 32, 10) done\n",
      "Best Score: 0.5725693106651306\n",
      "Best Params: {'conv_1_filters': 32, 'conv_2_filters': 64, 'conv_3_filters': 128, 'conv_4_filters': 128, 'conv_1_kernel': 3, 'conv_2_kernel': 3, 'conv_3_kernel': 3, 'conv_4_kernel': 3, 'dropout_1': 0.1, 'dropout_2': 0.0, 'dropout_3': 0.0, 'dense_units': 512, 'optimizer': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 23:24:26.831029: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_4/dropout_12/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 128, 128, 3, 3, 3, 3, 0.1, 0.0, 0.0, 1024, 'adam', 32, 10) done\n",
      "Best Score: 0.5765619874000549\n",
      "Best Params: {'conv_1_filters': 32, 'conv_2_filters': 64, 'conv_3_filters': 128, 'conv_4_filters': 128, 'conv_1_kernel': 3, 'conv_2_kernel': 3, 'conv_3_kernel': 3, 'conv_4_kernel': 3, 'dropout_1': 0.1, 'dropout_2': 0.0, 'dropout_3': 0.0, 'dense_units': 1024, 'optimizer': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 23:25:37.281540: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_5/dropout_15/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 128, 128, 3, 3, 3, 3, 0.1, 0.0, 0.2, 512, 'adam', 32, 10) done\n",
      "Best Score: 0.5906981825828552\n",
      "Best Params: {'conv_1_filters': 32, 'conv_2_filters': 64, 'conv_3_filters': 128, 'conv_4_filters': 128, 'conv_1_kernel': 3, 'conv_2_kernel': 3, 'conv_3_kernel': 3, 'conv_4_kernel': 3, 'dropout_1': 0.1, 'dropout_2': 0.0, 'dropout_3': 0.2, 'dense_units': 512, 'optimizer': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 23:26:47.249360: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_6/dropout_18/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 128, 128, 3, 3, 3, 3, 0.1, 0.0, 0.2, 768, 'adam', 32, 10) done\n",
      "Best Score: 0.5906981825828552\n",
      "Best Params: {'conv_1_filters': 32, 'conv_2_filters': 64, 'conv_3_filters': 128, 'conv_4_filters': 128, 'conv_1_kernel': 3, 'conv_2_kernel': 3, 'conv_3_kernel': 3, 'conv_4_kernel': 3, 'dropout_1': 0.1, 'dropout_2': 0.0, 'dropout_3': 0.2, 'dense_units': 512, 'optimizer': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 23:27:57.635618: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_7/dropout_21/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 128, 128, 3, 3, 3, 3, 0.1, 0.0, 0.2, 1024, 'adam', 32, 10) done\n",
      "Best Score: 0.5906981825828552\n",
      "Best Params: {'conv_1_filters': 32, 'conv_2_filters': 64, 'conv_3_filters': 128, 'conv_4_filters': 128, 'conv_1_kernel': 3, 'conv_2_kernel': 3, 'conv_3_kernel': 3, 'conv_4_kernel': 3, 'dropout_1': 0.1, 'dropout_2': 0.0, 'dropout_3': 0.2, 'dense_units': 512, 'optimizer': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 23:29:09.047680: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_8/dropout_24/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 128, 128, 3, 3, 3, 3, 0.1, 0.0, 0.3, 512, 'adam', 32, 10) done\n",
      "Best Score: 0.5906981825828552\n",
      "Best Params: {'conv_1_filters': 32, 'conv_2_filters': 64, 'conv_3_filters': 128, 'conv_4_filters': 128, 'conv_1_kernel': 3, 'conv_2_kernel': 3, 'conv_3_kernel': 3, 'conv_4_kernel': 3, 'dropout_1': 0.1, 'dropout_2': 0.0, 'dropout_3': 0.2, 'dense_units': 512, 'optimizer': 'adam'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 23:30:18.471026: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_9/dropout_27/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 64, 128, 128, 3, 3, 3, 3, 0.1, 0.0, 0.3, 768, 'adam', 32, 10) done\n",
      "Best Score: 0.5906981825828552\n",
      "Best Params: {'conv_1_filters': 32, 'conv_2_filters': 64, 'conv_3_filters': 128, 'conv_4_filters': 128, 'conv_1_kernel': 3, 'conv_2_kernel': 3, 'conv_3_kernel': 3, 'conv_4_kernel': 3, 'dropout_1': 0.1, 'dropout_2': 0.0, 'dropout_3': 0.2, 'dense_units': 512, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'conv_1_filters': [32, 64],\n",
    "    'conv_2_filters': [64, 96, 128],\n",
    "    'conv_3_filters': [128, 192, 224],\n",
    "    'conv_4_filters': [128, 160, 224, 256],\n",
    "    'conv_1_kernel': [3, 5],\n",
    "    'conv_2_kernel': [3, 5],\n",
    "    'conv_3_kernel': [3, 5],\n",
    "    'conv_4_kernel': [3, 5],\n",
    "    'dropout_1': [0.1, 0.2, 0.4],\n",
    "    'dropout_2': [0.0, 0.2, 0.3],\n",
    "    'dropout_3': [0.0, 0.2, 0.3, 0.4],\n",
    "    'dense_units': [512, 768, 1024],\n",
    "    'optimizer': ['adam'],\n",
    "    'batch_size': [32],  # Example, adjust as needed\n",
    "    'epochs': [10]  # Example, adjust as needed\n",
    "}\n",
    "\n",
    "def evaluate_model(params):\n",
    "    fit_params = {k: params.pop(k) for k in ['batch_size', 'epochs']}\n",
    "    \n",
    "    model = build_grid_model(**params)\n",
    "    model.fit(X_train, y_train, **fit_params, verbose=0)\n",
    "    score = model.evaluate(X_val, y_val, verbose=0)\n",
    "    return score[1]\n",
    "\n",
    "best_score = 0\n",
    "best_params = None\n",
    "\n",
    "for combo in itertools.product(*(param_grid[key] for key in param_grid)):\n",
    "    params = dict(zip(param_grid.keys(), combo))\n",
    "    accuracy = evaluate_model(params)\n",
    "    print(f\"{combo} done\")\n",
    "    if accuracy > best_score:\n",
    "        best_score = accuracy\n",
    "        best_params = params\n",
    "        print(\"Best Score:\", best_score)\n",
    "        print(\"Best Params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a15542-0ad5-49fa-add9-212fe3aaa5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best Score:\", best_score)\n",
    "print(\"Best Params:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c1e164",
   "metadata": {},
   "source": [
    "# Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "98927b23-a08a-48b3-b37d-af610a42ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simplified_model(conv_1_filters, conv_2_filters, \n",
    "                           conv_1_kernel, conv_2_kernel,\n",
    "                           dropout_1, dropout_2, dense_units, optimizer):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(conv_1_filters, kernel_size=conv_1_kernel, activation='relu', input_shape=(48,48,1)))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(dropout_1))\n",
    "\n",
    "    model.add(Conv2D(conv_2_filters, kernel_size=conv_2_kernel, activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(dropout_2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(dense_units, activation='relu'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fbe84aeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 04:03:04.136820: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_15/dropout_37/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "307/676 [============>.................] - ETA: 2s - loss: 1.9313 - accuracy: 0.1720"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 15\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m build_simplified_model(\n\u001b[1;32m      6\u001b[0m     conv_1_filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, conv_2_filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, \n\u001b[1;32m      7\u001b[0m     conv_1_kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, conv_2_kernel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m )\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     18\u001b[0m score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/engine/training.py:1813\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1811\u001b[0m logs \u001b[38;5;241m=\u001b[39m tmp_logs\n\u001b[1;32m   1812\u001b[0m end_step \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m+\u001b[39m data_handler\u001b[38;5;241m.\u001b[39mstep_increment\n\u001b[0;32m-> 1813\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m   1815\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/callbacks.py:475\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;124;03m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 475\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mModeKeys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTRAIN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/callbacks.py:322\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m hook \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_end_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    325\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized hook: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected values are [\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    327\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/callbacks.py:345\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_start_time\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times\u001b[38;5;241m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 345\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_batch_hook_helper\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_times) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    348\u001b[0m     end_hook_name \u001b[38;5;241m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/callbacks.py:393\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    392\u001b[0m     hook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 393\u001b[0m     \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_timing:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hook_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/callbacks.py:1093\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m-> 1093\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/callbacks.py:1169\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m add_seen\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;66;03m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1169\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[43mtf_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msync_to_numpy_or_python_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprogbar\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen, \u001b[38;5;28mlist\u001b[39m(logs\u001b[38;5;241m.\u001b[39mitems()), finalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:694\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mndim(t) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m t\n\u001b[0;32m--> 694\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_single_numpy_or_python_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/nest.py:631\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnest.map_structure\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_structure\u001b[39m(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    547\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    548\u001b[0m \n\u001b[1;32m    549\u001b[0m \u001b[38;5;124;03m  Refer to [tf.nest](https://www.tensorflow.org/api_docs/python/tf/nest)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;124;03m    ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1066\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(modality, func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates a new structure by applying `func` to each atom in `structure`.\u001b[39;00m\n\u001b[1;32m    970\u001b[0m \n\u001b[1;32m    971\u001b[0m \u001b[38;5;124;03m- For Modality.CORE: Refer to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m  ValueError: If wrong keyword arguments are provided.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1066\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1068\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure(func, \u001b[38;5;241m*\u001b[39mstructure, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1106\u001b[0m, in \u001b[0;36m_tf_core_map_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m   1101\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1102\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1106\u001b[0m     [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1107\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1108\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1106\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1101\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (_tf_core_flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m   1102\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1105\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m-> 1106\u001b[0m     [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m   1107\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1108\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/tf_utils.py:687\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    684\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    685\u001b[0m     \u001b[38;5;66;03m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[1;32m    686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, tf\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m--> 687\u001b[0m         t \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Strings, ragged and sparse tensors don't have .item(). Return them\u001b[39;00m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# as-is.\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(t, (np\u001b[38;5;241m.\u001b[39mndarray, np\u001b[38;5;241m.\u001b[39mgeneric)):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:394\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \n\u001b[1;32m    373\u001b[0m \u001b[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m--> 394\u001b[0m maybe_arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_arr\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(maybe_arr, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;28;01melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:360\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_numpy\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    359\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 360\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_numpy_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    361\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "import os\n",
    "os.environ['TF_DISABLE_OPTIMIZATION'] = 'layout'\n",
    "\n",
    "model = build_simplified_model(\n",
    "    conv_1_filters=32, conv_2_filters=64, \n",
    "    conv_1_kernel=3, conv_2_kernel=3, \n",
    "    dropout_1=0.5, dropout_2=0.6, \n",
    "    # dropout_1=0.05, dropout_2=0.3, \n",
    "    dense_units=1024, \n",
    "    optimizer='adam'\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=7, validation_data=val_dataset, class_weight=class_weight_dict)\n",
    "\n",
    "# Evaluate the model\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e6f3fb9e-ef57-49ed-b02d-7d86ec690068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "676/676 [==============================] - 6s 8ms/step - loss: 1.8669 - accuracy: 0.2239 - val_loss: 1.7330 - val_accuracy: 0.3453\n",
      "Epoch 2/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.6543 - accuracy: 0.3583 - val_loss: 1.6205 - val_accuracy: 0.3683\n",
      "Epoch 3/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.5356 - accuracy: 0.4115 - val_loss: 1.4869 - val_accuracy: 0.4286\n",
      "Epoch 4/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.4358 - accuracy: 0.4490 - val_loss: 1.3880 - val_accuracy: 0.4771\n",
      "Epoch 5/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.3312 - accuracy: 0.4877 - val_loss: 1.3536 - val_accuracy: 0.4917\n",
      "Epoch 6/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.2553 - accuracy: 0.5194 - val_loss: 1.3066 - val_accuracy: 0.5119\n",
      "Epoch 7/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.1700 - accuracy: 0.5464 - val_loss: 1.2792 - val_accuracy: 0.5192\n",
      "194/194 [==============================] - 1s 3ms/step - loss: 0.8861 - accuracy: 0.7216\n",
      "Epoch 1/7\n",
      "676/676 [==============================] - 6s 8ms/step - loss: 1.8766 - accuracy: 0.2199 - val_loss: 1.7305 - val_accuracy: 0.3406\n",
      "Epoch 2/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.6693 - accuracy: 0.3509 - val_loss: 1.5777 - val_accuracy: 0.4083\n",
      "Epoch 3/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.5304 - accuracy: 0.4083 - val_loss: 1.4526 - val_accuracy: 0.4481\n",
      "Epoch 4/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.4312 - accuracy: 0.4493 - val_loss: 1.3763 - val_accuracy: 0.4831\n",
      "Epoch 5/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.3353 - accuracy: 0.4828 - val_loss: 1.3330 - val_accuracy: 0.4944\n",
      "Epoch 6/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.2653 - accuracy: 0.5183 - val_loss: 1.3151 - val_accuracy: 0.5106\n",
      "Epoch 7/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.1803 - accuracy: 0.5485 - val_loss: 1.2699 - val_accuracy: 0.5208\n",
      "194/194 [==============================] - 1s 3ms/step - loss: 1.2187 - accuracy: 0.5707\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 04:04:41.798779: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_18/dropout_43/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676/676 [==============================] - 6s 8ms/step - loss: 1.8455 - accuracy: 0.2409 - val_loss: 1.7279 - val_accuracy: 0.3321\n",
      "Epoch 2/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.6246 - accuracy: 0.3652 - val_loss: 1.5278 - val_accuracy: 0.4430\n",
      "Epoch 3/7\n",
      "676/676 [==============================] - 5s 8ms/step - loss: 1.4999 - accuracy: 0.4205 - val_loss: 1.4607 - val_accuracy: 0.4475\n",
      "Epoch 4/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.3980 - accuracy: 0.4656 - val_loss: 1.3689 - val_accuracy: 0.4857\n",
      "Epoch 5/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.3077 - accuracy: 0.4970 - val_loss: 1.3145 - val_accuracy: 0.5048\n",
      "Epoch 6/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.2187 - accuracy: 0.5298 - val_loss: 1.2852 - val_accuracy: 0.5228\n",
      "Epoch 7/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.1218 - accuracy: 0.5690 - val_loss: 1.2415 - val_accuracy: 0.5391\n",
      "194/194 [==============================] - 1s 3ms/step - loss: 1.1917 - accuracy: 0.5453\n",
      "Epoch 1/7\n",
      "676/676 [==============================] - 6s 8ms/step - loss: 1.8902 - accuracy: 0.2102 - val_loss: 1.7590 - val_accuracy: 0.3412\n",
      "Epoch 2/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.7088 - accuracy: 0.3324 - val_loss: 1.5994 - val_accuracy: 0.3898\n",
      "Epoch 3/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.5521 - accuracy: 0.4005 - val_loss: 1.4434 - val_accuracy: 0.4642\n",
      "Epoch 4/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.4420 - accuracy: 0.4470 - val_loss: 1.3667 - val_accuracy: 0.4884\n",
      "Epoch 5/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.3404 - accuracy: 0.4838 - val_loss: 1.3254 - val_accuracy: 0.5048\n",
      "Epoch 6/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.2491 - accuracy: 0.5186 - val_loss: 1.3084 - val_accuracy: 0.5078\n",
      "Epoch 7/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.1747 - accuracy: 0.5497 - val_loss: 1.3123 - val_accuracy: 0.5114\n",
      "194/194 [==============================] - 1s 3ms/step - loss: 1.2268 - accuracy: 0.5732\n",
      "Epoch 1/7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-28 04:05:56.436280: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_20/dropout_47/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676/676 [==============================] - 6s 8ms/step - loss: 1.8427 - accuracy: 0.2475 - val_loss: 1.6595 - val_accuracy: 0.3941\n",
      "Epoch 2/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.6179 - accuracy: 0.3690 - val_loss: 1.5554 - val_accuracy: 0.4103\n",
      "Epoch 3/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.4776 - accuracy: 0.4278 - val_loss: 1.3985 - val_accuracy: 0.4731\n",
      "Epoch 4/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.3717 - accuracy: 0.4727 - val_loss: 1.3601 - val_accuracy: 0.4888\n",
      "Epoch 5/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.2740 - accuracy: 0.5113 - val_loss: 1.3317 - val_accuracy: 0.5029\n",
      "Epoch 6/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.1902 - accuracy: 0.5402 - val_loss: 1.2682 - val_accuracy: 0.5297\n",
      "Epoch 7/7\n",
      "676/676 [==============================] - 5s 7ms/step - loss: 1.0953 - accuracy: 0.5775 - val_loss: 1.2504 - val_accuracy: 0.5309\n",
      "194/194 [==============================] - 1s 3ms/step - loss: 0.8740 - accuracy: 0.6897\n",
      "Average Score: [1.07945907 0.62009355]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X and y are your data and labels\n",
    "kf = KFold(n_splits=5)\n",
    "results = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    # Build and train your model here\n",
    "    model = build_simplified_model(\n",
    "        conv_1_filters=32, conv_2_filters=64, \n",
    "        conv_1_kernel=3, conv_2_kernel=3, \n",
    "        dropout_1=0.5, dropout_2=0.6, \n",
    "        # dropout_1=0.05, dropout_2=0.3, \n",
    "        dense_units=1024, \n",
    "        optimizer='adam'\n",
    "    )\n",
    "    model.fit(train_dataset, epochs=7, validation_data=val_dataset, class_weight=class_weight_dict)\n",
    "\n",
    "    # Evaluate the model\n",
    "    score = model.evaluate(X_val, y_val)\n",
    "    results.append(score)\n",
    "\n",
    "# Calculate average of the results\n",
    "average_score = np.mean(results, axis=0)\n",
    "print(\"Average Score:\", average_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4270ebed-739c-4512-ac73-e9f0feedd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('FER_v1.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad972895-fc4e-4305-a542-61e0b57c76dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
